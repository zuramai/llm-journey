# Glossary

List of terms I found and its meaning.

### Hallucination

Hallucination is when a model generate something that looks like a coherent and grammatically correct output, but it actually incorrect and nonsensical. LLM has this limitation because of not enough training data. It can generate grammatically correct sentence because it exposed to a lot of training data in that target language.